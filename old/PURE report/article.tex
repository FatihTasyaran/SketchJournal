\documentclass[twoside]{article}
\usepackage{float}
\usepackage{graphicx}
\usepackage{framed}
\usepackage[a4paper, total={6.4in, 8in}]{geometry}
\usepackage[top=5cm, bottom=0.3cm, left=5cm, right=3cm]{geometry}

\usepackage{multicol} % This is so we can have multiple columns of text side-by-side
\columnsep=100pt % This is the amount of white space between the columns in the poster
\columnseprule=15pt % This is the thickness of the blue line between the columns in the poster
\usepackage{subfig}
\usepackage{float}
\usepackage[svgnames]{xcolor} % Specify colors by their 'svgnames', for a full list of all colors available see here: http://www.latextemplates.com/svgnames-colors
%\usepackage{times} % Use the times font
\usepackage{palatino} % Uncomment to use the Palatino font
\usepackage{xkeyval}
\usepackage{graphicx} % Required for including images
\setlength{\abovecaptionskip}{5pt plus 5pt minus 3pt}
\graphicspath{{figures/}} % Location of the graphics files
\usepackage{booktabs} % Top and bottom rules for table
\usepackage[font=small,labelfont=bf]{caption} % Required for specifying captions to tables and figures
\usepackage{amsfonts, amsmath, amsthm, amssymb} % For math fonts, symbols and environments
\usepackage{csquotes}
\usepackage{wrapfig} % Allows wrapping text around tables and figures

\usepackage{amsmath}
\usepackage{subfig}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\pagestyle{fancy}

\fancypagestyle{firststyle}
{
	\fancyhead{}
	\fancyhead[L]{Sabancı University Program for Undergraduate Research (PURE)}
		\fancyhead[R]{Fall 2017-2018}
}
\fancyhead{}
\fancyhead[CE]{Yıldırır,Taşyaran}
\fancyhead[CO]{Analysis of Data Sketches}

\setlength{\oddsidemargin}{\evensidemargin}  

\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{0pt}
%\usepackage{biblatex}
%\addbibresource{References.bib}
\bibliography{bibliography}
\usepackage[
style=apa,
backend=biber,
sortcites=true,
sorting=nyt,
%    isbn=false,
%    url=false,
%    doi=false,
%    eprint=false,
hyperref=false,
backref=false,
%    firstinits=false,
]{biblatex}

\begin{document}
	\thispagestyle{firststyle}
	\begin{figure}
	

\textbf{Fatih Taşyaran} \hfill fatihtasyaran@sabanciuniv.edu \\
\hfill	\textit{Computer Science and Engineering,2014} \\
\textbf{Kerem Yıldırır} \hfill keremyildirir@sabanciuniv.edu \\
\hfill	\textit{Computer Science and Engineering,2015} \\
\textbf{Kamer Kaya} \par
\textit{Computer Science and Engineering}
	
	\end{figure}
	\section{Abstract}
	One of the biggest challenges in today's business and technology world is dealing with enormous chunks of data. One of the problems people encounter while working with big data is that it takes too much time to analyze it. Standard data structures like linked lists and hash tables may be incompetent or inefficient for some specific problems such as keeping track of the elements' frequency, or checking if an element is in the set or not. Hence, people are trying  to come up with clever solutions such as data sketches. We implemented various sketches, and analyzed their performance for possible improvements.
\section{Introduction}

Data sketches are derived from Bloom Filters, which are arrays that consist of 1 and 0s (Cormode, 2017). Bloom Filters are used in search and validation purposes. Bloom Filters(BF) can answer a query with 2 possible outputs: \begin{itemize}
	\item The element is definitely not in the data
	\item The element is probably in the data 
	
\end{itemize}
 In ideal case, for $i$th element in the data, $BF[i]=1$. On the contrary, in real data, this is rarely the case. Even on hash table applications, there exists an item $i$ such that $h_{i}=h_{j}$. Still,
 BM's are compact, nice structures. (Cormode, 2017)


Sketches are data structures which are used for following the frequencies of elements in a large data set. Main distinguishing feature of a sketch is that its size does not depend on size of the data set. Size of the sketch depends on two error parameters: $\delta$ and $\epsilon$, where $\epsilon$ is the minimum error rate  user can afford
and $\delta$ is the error frequency. 
Number of rows and columns in the sketch is computed with the following formulas: $$nrows=\log_{2}(\frac{1}{\delta})$$ 
$$ncols=\frac{2}{\epsilon}$$
A step by step explanation for sketch mechanism is  as follows:
\begin{itemize}
\item Define sketch size with $\delta$ and $\epsilon$
\item Insert elements into the sketch table using predefined number of hash functions and perform the operation on table in order to keep the frequency. (Different sketches use different operations.(Cormode, 2017))

\end{itemize}
There are various hash functions for this purpose, but in this paper, our choice was Tabular Hashing. The reason behind this choice  is ,Tabular Hashing is simple, robust and easily implemented. We managed to improve total performance by merging the hash functions' tables, which led to a significant improvement performance-wise.
\section{Implemented Data Sketches}
\subsection{Count-Min Sketch}
In the application of this sketch, the element to be inserted is hashed $n$ times, where $n$ represents the number of rows. Each element is hashed by hash functions $h_{1},h_{2} \dots h_{n}$. Each hash function increments  $sketch[i][h_{i}]$ where $i$ corresponds to  hash function's return value and $sketch$ refers to the sketch table. For queries, the minimum corresponding value of all rows is returned.(Cormode, 2017)
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{Countmin.png}
	\caption{Illustration of Count-Min Sketch}
	

\end{figure}
\subsection{Count-Mean Min Sketch}
Initialization of Count-Mean Min Sketch(CMM) is exactly same as Count-Min Sketch(CM). It can be said that since every index is incremented for multiple elements, CM's results are \textit{overestimating} the true values. Hence it is a biased estimator. CMM intends to solve this problem by subtracting a noise from the queried value. Noise is estimated as : $\frac{NoElements - sketch[i][h_{i}]}{ncols- 1}  $ for every row. After calculating the noise, it is deducted from each $sketch[i][h_{i}]$ value computed in each row. Finally, the minimum of these values is returned as query output.$$
output=\min(sketch[i][h_{i}]-noise_{i})\quad i=1 \dots nrows$$ where $i$ is the row number and $h_ {i}$ is the index obtained from hash function. (Deng,Rafiei, n.d)


\subsection{Count Sketch}
In Count Sketch, 2 hash functions are maintained for each row. $h_{k}$ for determining the column index value, and $g_{k}$ for determining an increment value in the interval $[-1,1]$.is done by acquiring the median of all $sketch[i][h_{i}]$.
$$sketch[k][h_{k}]=sketch[k][h_{k}]+ g_{k} \quad  k=1 \dots nrow$$
It aims to neutralize the bias that comes with overestimating. (Cormode,Hadjieleftheriou, 2008)
\subsection{Count-Min Sketch with Conservative Update }
This is an update for Count-Min Sketch and aims to solve the unnecessary incrementation in the sketch. Since queries return the minimum of $sketch[i][h_{i}]$, during the incrementation, it only increments the minimum value of all $sketch[i][h_{i}]$. In other words, only $\min (sketch[i][h_{i}])$ is incremented. This heuristic reduces the bias caused by overestimating.(Goyal, 2012)
\section{Tabular Hashing}
In data sketches, elements that inserted to the  table must be hashed multiple times. In this implementation of sketching, tabular hashing is used. Which first introduced in an article by Zobrist to avoid recalculations of board configurations at the current state in board games.(Thorup, 2012) It is a  strong hash function which grants significant level of randomness. Hashing  process requires a table filled with random integers.  After  initializing the table, hash function can be used. Code for hashing can be
examined in the following figure.
\begin{figure}[H]
	\centering
\includegraphics[width=0.5\textwidth]{hashhh.png}
\caption{Hash function written in C++}
\label{fig:hash}
\end{figure}

With using the fastest known hashing scheme multiplication-shift hashing. Hash function takes an 32-bit integer $x$ as input. It divides $x$ into 4 8-bit chunks and iterates them in order. At each iteration, logical AND operation is done between the element and an 8-bit mask which consists of zeros.
With this, it is guaranteed that first 8-bits of the 32-bit integer becomes zero. This also changes the value of element $x$, to another integer $c$. Then, logical XOR operation is applied between changed element and the integer in hash table which is on the $ith$ row where $0<i<4$ and $cth$ column and assigned to a variable $h$. At the end of iteration, $x$ is shifted right 8-bits. When all iterations are done a unique $h$ value has been achieved.

\begin{center}
	\includegraphics[width=\linewidth]{tabularHash}
	\captionof{figure}{\color{Green}Tabular Hash illustrated}
	\label{GIMlatlon}
\end{center}
At the end of iteration,  is shifted right 8-bits. When all iterations are done a unique $h$ value has been achieved. 


\section{Parallelization}
The sketch can be seen as a sum of arbitrary sketches using the same $h_{j}$ for their rows $j$. In this manner every sketch is combinable while they use same $\epsilon$, $\delta$ and hash function $h_{j}$ for their rows $j$.
Say in sketch $A$, $A[j,k]$ is the sum of updates to the location $(j,k)$ for index values $i$ such that $H_{j}(i) = k$ in the process and $a_{j,k}$ is a binary vector such that $a_{j,k} = 1$ if and only if $h_j(i) = k$.
Then we can say $$A[j,k] = \sum_{{i=0}}^{{n}}a_{n}[j,k]$$
(Cormode, 2017)
This led us an important consequence that sketches have high potential of parallelization. Since the data is shared between threads that all have their own sketch then combined in order to form a final table by OpenMP's fork-joint parallelism, we can distribute the input set to the threads, as many as available.
\par The drawback here is, while the threads are increasing and subsets of the input set that computed parallel are increasing, memory required to contain their tables are also increasing. This called time-memory trading. In our project, we focus on multi-table computing of sketches, we use more memory but get improvements on performance.

\subsection{Synchronization Problems With Parallelization}
\subsubsection{Race Condition}
In multi-thread programs, threads are communicate by sharing variables. In cache, threads are interleaving. Basically saying, there is no defined schedule for which thread is processing at the time or in which sequence they will be processed. Thus when the scheduling of threads affect the output, this ambiguity may result in different outputs of the same program at different runs. This is called a race condition.
\subsubsection{False Sharing}
In symmetric multiprocessor systems, each processor has a local cache. And the memory system must guarantee a consistency between the data in this caches, called cache coherency protocol. When threads on different processors modify variables that sits on the same cache line, a false sharing occurs. This invalidates the cache line and forces an update on cache line. Thus the false sharing causes cache lines to go back and forth between threads and decreases performance.
\subsubsection{Synchronization Tools}
\textbf{Synchronization} is bringing one or more threads to a well defined and known point in their execution. Common synchronization tools are:
\begin{itemize}
	\item \textbf{Barrier:} Each thread wait at the barrier until all threads arrive.
	\item \textbf{Mutual Exclusion:} Defining a block of code that only one thread at a time can execute.
	\item \textbf{Critical:} Defines a mutual exclusion for a block of code.
	\item \textbf{Atomic:} Uses hardware support to do a quick update on a value. If there is no hardware support, act like critical. Only use a scalar l-value and a non overloaded built-in operator.
	\item \textbf{Lock:} Lowest level mutual exclusion in synchronization. Lock is actually a variable. The thread that holds the lock can execute processes on the lock and the other threads doing something else while waiting for lock is available. 
	
\end{itemize}

\section{Parallelization With Synchronized Hashing}
As mentioned before, we focus on multi-table sketches in our project and gain performance while using more memory. Moreover we develop a new way of tabular hashing in order to exploit the speed of fastest L1 Cache. In our model of tabular hash, instead of using different randomly-filled tables for each row of the sketch as a look-up table for corresponding row's hash function; we merged these tables by arrange the columns of tables of rows consecutively.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{mergedhasher.png}
	\caption{Code for merged tabular hashing}
	
	\label{fig:code}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{illustration.png}
	\caption{Illustration of Merging Tables of Rows}
	
	
\end{figure}

\par 
In the figure \ref{fig:code}, implementation of merged hasher can be seen. Since row size has been fixed to 8, results of hash functions are put side by side in the hash table.
The advantage of our method is, instead of different threads to read data that sits on different cache lines, all threads read data from the same cache line. That will be on the fastest L1 cache during process. Expecting result is reducing in cache-misses thus increase in performance.
This result could be seen in experiments section.
\section{Batching and Padding}
Furthermore, we determined which part of the stream will be processed by which thread. Hence, instead of processing the data as a whole, we divided the stream into batches of 1024 elements. This way, we were able to assign the data to the threads in a way such that each thread writes to it's predetermined location on the sketch without overlapping, eliminating race conditions. As expected, our throughput increased significantly especially when the sketch size is large(8 by 2003).
\par
Since we desired to see the same improvement also in a smaller sketch(8 by 211), we implemented a padding over the buffers which keep the results from hashing. To keep eight 32-bit integers, we used an array of 256 32-bit integers. With this way, we were able to provide a better cache line integrity, resulting in fewer number of threads processing on the same cache line. 

\section{Experiments}

We ran our experiments, on a Raspberry Pi 3 Model B which has the following specs:

\begin{framed}

	\textbf{SoC}: Broadcom BCM2837\par
	\textbf{CPU}: 4x ARM Cortex-A53, 1.2GHz\par
	\textbf{GPU}: Broadcom VideoCore IV\par
	\vspace{1mm}
	\textbf{RAM}: 1GB LPDDR2 (900 MHz)\par
	\textbf{Networking}: 10/100 Ethernet, 2.4GHz 802.11n wireless\par
	\textbf{Bluetooth}: Bluetooth 4.1 Classic, Bluetooth Low Energy\par
	\textbf{Storage}: 16 GB Class10 microSD\par
	\textbf{GPIO}: 40-pin header, populated\par
	\textbf{Ports}: HDMI, 3.5mm analogue audio-video jack, 4 x USB 2.0, Ethernet, Camera Serial Interface (CSI), Display Serial Interface (DSI)
\end{framed}

To calculate the speed-up we used the following formula:

\par $$T_{s} =\text{Non-parallel execution time}$$
$$T_{n} = \text{Execution time using $n$ threads} $$
$$SpeedUp = \frac{T_{s}}{T_{n}}$$

We also fixed our sketches size to 8 by 211 in order to facilitate coding and track results more precisely while developing the new method.

\section*{Results and Conclusion}
To assess our improvement's success, we ran experiments with every consecutive version we optimized in order to provide a better cache utilization.
\vspace{0.1cm}
\begin{center}
	\includegraphics[width=0.7\linewidth]{throughs2}
	\captionof{figure}{\color{Green}Throughputs of Different Versions Averaged for 10 Experiments}
	\label{GIMlatlon}
\end{center}
Multi table is what we've started with, then we merged the hash tables to achieve a better spatial locality. Next step was to made threads work on a single sketch. Low performance on single sketch is due to false sharing caused by threads. To overcome this we batched the stream and padded the medium result arrays.
\vspace{0.1cm}
\begin{center}
	\includegraphics[width=0.7\linewidth]{times2}
	\captionof{figure}{\color{Green}Completion Times of Different Versions Averaged for 10 Experiments}
	\label{GIMlatlon}
\end{center}
Finally, reduced cache miss rates based on the well determined workspaces of threads on the sketch and stream led to a visible performance increase on task.
\section*{Future Work and Further Improvement}
For future improvement, our aim is to increase the performance even more, we plan to run our application on other single board computers with multiple cores such as Odroid and Parallela. For the purpose of reproducible researching, source code of the project will be posted on BitBucket and GitHub .
\section{References}
\begin{itemize}
\item Goyal, A., Daume, H.,  Cormode, G. (2012, July). Sketch Algorithms for Estimating Point Queries in NLP. Retrieved from http://www.umiacs.umd.edu/~amit/Papers/goyalPointQueryEMNLP12.pdf
\item Pǎtraşcu, M.,  Thorup, M. (2012). The Power of Simple Tabulation Hashing. Journal of the ACM, 59(3), 1-50. doi:10.1145/2220357.2220361
\item Thorup, M. (2017). Fast and powerful hashing using tabulation. Communications of the ACM, 60(7), 94-101. doi:10.1145/3068772

\item Cormode, G. (2017). Data sketching. Communications of the ACM, 60(9), 48-55. doi:10.1145/3080008 

\item 
Cormode, G., \& Hadjieleftheriou, M. (2008, August). Finding Frequent Items in Data Streams. AT\&T Labs–Research. Retrieved from http://www.vldb.org/pvldb/1/1454225.pdf
\item

Deng, F., \& Rafiei, D. (n.d.). New Estimation Algorithms for Str eaming Data: Count-min Can Do More. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.420.449\&rep=rep1\&type=pdf


\end{itemize}

\end{document}